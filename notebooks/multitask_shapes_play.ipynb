{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4607ae",
   "metadata": {},
   "source": [
    "# Multitask Shapes – interactive play  \n",
    "  \n",
    "This notebook lets you **play the customised “Multitask Shapes” environment** that you pasted.  \n",
    "At each step you’ll be asked for an action:\n",
    "\n",
    "| index | meaning  |\n",
    "|-------|----------|\n",
    "| 0     | **UP**   |\n",
    "| 1     | **DOWN** |\n",
    "| 2     | **LEFT** |\n",
    "| 3     | **RIGHT**|\n",
    "| 4     | **PICK** |\n",
    "| 5     | **DROP** |\n",
    "\n",
    "The notebook will  \n",
    "* execute the action,  \n",
    "* print the new reward & raw observation, and  \n",
    "* display the rendered frame.\n",
    "\n",
    "> **Prerequisites**  \n",
    "> * `shapes.py`, `multitask_shapes.py`, `utils.py`, the `assets/` folder and your YAML hyper‑parameter file must be provided.\n",
    "> * Python ≥ 3.9 with the packages in the first code‑cell installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2be1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# (Optional) install runtime dependencies\n",
    "# ------------------------------------------------------------\n",
    "# If you are missing any of these packages, remove the leading\n",
    "# '#' and run the cell once.\n",
    "#\n",
    "# !pip install -q gymnasium numpy opencv-python-headless matplotlib pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faab382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djordje/miniconda3/envs/rlexp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Imports & utility helpers\n",
    "# ------------------------------------------------------------\n",
    "import os, yaml, numpy as np, cv2, matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Adjust the import below if your python file has a different name\n",
    "from envs.shapes.multitask_shapes import MultitaskShapes, ShapesPositionFactory\n",
    "from utils import setup_artefact_paths\n",
    "\n",
    "# Helper to display a rendered frame inline\n",
    "def show_frame(img):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    # convert BGR (OpenCV) ➜ RGB (matplotlib)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3338a57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready ✔\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Load hyper‑parameters & create the environment\n",
    "# ------------------------------------------------------------\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "# Fill yaml_path as desired, the rest should work as is.\n",
    "yaml_path = os.path.join(parent_dir, \"envs\", \"shapes\", \"configs\", \"shapes.yaml\")\n",
    "\n",
    "script_path = os.path.join(os.getcwd(), 'multitask_shapes_play.ipynb')\n",
    "store_path, _ = setup_artefact_paths(script_path)\n",
    "\n",
    "with open(yaml_path, 'r') as f:\n",
    "    hparams = yaml.safe_load(f)\n",
    "\n",
    "hparams = hparams[\"environment\"] if \"environment\" in hparams else hparams\n",
    "relevant_feature_index = 1 + int(hparams[\"goal_channel\"])\n",
    "\n",
    "env = ShapesPositionFactory(\n",
    "    hparams=hparams,\n",
    "    store_path=store_path\n",
    ").get_env(set_id=\"TRAIN\")\n",
    "obs_type = hparams[\"obs_type\"]\n",
    "print('Environment ready ✔')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450011d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1\n",
      "Return so far: -1\n",
      "Reward from previous action: -1\n",
      "Current observation: [3 6 2 6 1 3 1 1 4 4 2 6]\n",
      "Instruction: Go to green ball.\n",
      "Choose action – 0:UP  1:DOWN  2:LEFT  3:RIGHT  4:PICK UP  5:DROP OFF\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# -------- environment step ------------\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m next_obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# update bookkeeping\u001b[39;00m\n\u001b[1;32m     44\u001b[0m last_reward \u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/dev/rlexp/envs/shapes/multitask_shapes.py:177\u001b[0m, in \u001b[0;36mMultitaskShapes.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 177\u001b[0m     _, reward, is_terminal, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs, reward, is_terminal, truncated, info\n",
      "File \u001b[0;32m~/dev/rlexp/envs/shapes/shapes.py:369\u001b[0m, in \u001b[0;36mShapesGoto.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# Superclass will perform the agent movement but will not do any of the additional actions\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# or provide sensible reward\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     obs, _, _, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     confounder_locations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objects \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_goal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m obj \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_goal\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m    372\u001b[0m     is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/rlexp/envs/shapes/shapes.py:250\u001b[0m, in \u001b[0;36mShapes.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Perform an action in the environment. Actions are as follows:\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m        - 0: go up\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m        - 1: go down\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m        - 5: drop #not used for now\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_movement\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    252\u001b[0m     info \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/dev/rlexp/envs/shapes/shapes.py:231\u001b[0m, in \u001b[0;36mShapes._movement\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    229\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 231\u001b[0m agent_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_location) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_to_direction\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grid[agent_location] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_location \u001b[38;5;241m=\u001b[39m agent_location\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "# ---- initial reset ----\n",
    "obs, _ = env.reset()\n",
    "step = 0\n",
    "total_reward = 0\n",
    "last_reward = None          # stores reward from previous step\n",
    "terminated = truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # -------- show current frame ----------\n",
    "    frame = env.render_frame()\n",
    "    show_frame(frame)\n",
    "\n",
    "    # -------- show textual info -----------\n",
    "    print(f\"Step: {step}\")\n",
    "    print(f\"Return so far: {total_reward}\")\n",
    "    if last_reward is not None:\n",
    "        print(f\"Reward from previous action: {last_reward}\")\n",
    "    if obs_type == \"box\":\n",
    "        print(f\"Current observation: {obs['features'][relevant_feature_index:, 1, 1]}\")\n",
    "        print(f\"{obs['features'][0,:,:]}\")\n",
    "        print(f\"{obs['features'][1,:,:]}\")\n",
    "    else:\n",
    "        print(f\"Current observation: {obs['features']}\")\n",
    "\n",
    "    print(f\"Instruction: {obs['instr']}\")\n",
    "    print(\"Choose action – 0:UP  1:DOWN  2:LEFT  3:RIGHT  4:PICK UP  5:DROP OFF\")\n",
    "\n",
    "    # -------- get action ------------------\n",
    "    try:\n",
    "        action = int(input(\"Your action index: \").strip())\n",
    "        if action not in range(6):\n",
    "            raise ValueError\n",
    "    except ValueError:\n",
    "        print(\"❌  Please enter an integer 0‑5.\")\n",
    "        time.sleep(1)        # let the user read the error\n",
    "        continue\n",
    "\n",
    "    # -------- environment step ------------\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # update bookkeeping\n",
    "    last_reward = reward\n",
    "    total_reward += reward\n",
    "    obs = next_obs\n",
    "    step += 1\n",
    "\n",
    "# final display\n",
    "clear_output(wait=True)\n",
    "frame = env.render_frame()\n",
    "show_frame(frame)\n",
    "print(\"Episode finished.\")\n",
    "if obs_type == \"box\":\n",
    "    print(obs['features'][relevant_feature_index:, 1, 1])\n",
    "else:\n",
    "    print(f\"Current observation: {obs['features']}\")\n",
    "print(f\"Total steps: {step}   Return: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56dd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlexp",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
